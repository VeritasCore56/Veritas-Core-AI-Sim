# -*- coding: utf-8 -*-
"""SOLID999.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlQKLjCix4D_pPmSuDhmY2hl6ruATV4S
"""

import logging
import sys
from fractions import Fraction
import math
import itertools # For generating combinations of parameters
import operator # For using operators like +, -, *, /
import os # Import os module for path operations


# --- Configuration for saving output to file ---
output_directory = "/content/drive/MyDrive/" # This is your Google Drive root
output_filename = "veritas_core_full_log.txt"
full_output_path = os.path.join(output_directory, output_filename)

# --- Redirect sys.stdout to the file ---
original_stdout = sys.stdout
log_file = open(full_output_path, "w")
sys.stdout = log_file

# --- Reconfigure logging (now logging to the redirected stdout, which goes to file) ---
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

# --- Add a logger specific to the output redirection process itself ---
logger_output_redirect = logging.getLogger("Output_Redirect")
logger_output_redirect.info(f"All subsequent console output and logs will be saved to: {full_output_path}")


# Get specific loggers
logger_main = logging.getLogger("AI_Core")
logger_x8 = logging.getLogger("x8_Module")
logger_x6_plus_2 = logging.getLogger("x6+2_Module")
logger_coordinator = logging.getLogger("Coordinator_Module")
logger_validator = logging.getLogger("Validator_Module")
logger_derived_rels = logging.getLogger("DerivedRels_Module")
logger_meta_rule = logging.getLogger("MetaRule_Module")
logger_inventor = logging.getLogger("Inventor_Module")


# --- Global shared_data_register and ai_core_parameters ---
shared_data_register = {
    "current_x8_term": None,
    "current_x6_plus_2_term": None,
    "last_shared_output_value": None,
    "last_shared_output_source": None,
    "validation_flag": True,
    "errors_detected": [],
    "validation_context_k_index": 1,
    "goal_for_x6_plus_2_first_term": 10.0,
    "x6_plus_2_recent_terms_history": []
}

ai_core_parameters = {
    "BASE_FACTOR_A": 2.0,
    "BASE_FACTOR_B": 6.0,
    "BASE_FACTOR_C": 8.0,
    "BASE_FACTOR_D": 3.0,
}

# --- Initialized AI Value Hierarchy (as it would be in a full system setup) ---
ai_value_hierarchy = {
    "CORE_DERIVATIONS_DISHARMONY_WEIGHT": 1000.0,
    "DECIMAL_SERIES_DISHARMONY_WEIGHT": 100.0,
    "X6_PLUS_2_FIRST_TERM_DISHARMONY_WEIGHT": 10.0,
    "GENERIC_25K_DISHARMONY_WEIGHT": 5.0,
}

logger_main.info("Initialized AI Core Parameters: %s", ai_core_parameters)
logger_main.info("Initialized shared_data_register with learning goal: %s", shared_data_register)
logger_main.info("Initialized AI Value Hierarchy: %s", ai_value_hierarchy)


# --- Symbolic Representation of Core Formulas ---
veritas_core_formulas = {
    "x6+2_sequence": ('odd_num', '*', "BASE_FACTOR_B", '+', "BASE_FACTOR_A"),
}
logger_main.info("Initialized Veritas Core Formulas (symbolic): %s", veritas_core_formulas)


# --- Define ALL FUNCTIONS (RE-ORDERED AND FORMAT-FIXED) ---

def calculate_digital_root(n: int) -> int:
    n_int = int(round(n))
    if n_int == 0: return 0
    return (n_int - 1) % 9 + 1

def generate_veritas_lux_chart() -> list[list[int]]:
    chart = []
    for r in range(1, 11):
        row_values = []
        for c in range(1, 11):
            col_base = c if c <= 9 else 1
            product = r * col_base
            digital_root_product = calculate_digital_root(product)
            row_values.append(digital_root_product)
        chart.append(row_values)
    return chart

def generate_x8_progression_term(n: int) -> int:
    if n < 1: raise ValueError("n must be a positive integer starting from 1.")
    factor_C = ai_core_parameters["BASE_FACTOR_C"]
    term = (2 * n - 1) * factor_C
    if term in [8, 56, 104]:
        shared_data_register["last_shared_output_value"] = term
        shared_data_register["last_shared_output_source"] = "x8_progression"
        logger_x8.info("Detected Shared Output: %s", term)
    shared_data_register["current_x8_term"] = term
    return int(round(term))

def generate_x6_plus_2_sequence_term(n: int) -> int:
    if n < 1: raise ValueError("n must be a positive integer starting from 1.")

    current_formula_tuple = veritas_core_formulas["x6+2_sequence"]
    odd_num_val = (2 * n - 1)

    expression_parts = []
    for part in current_formula_tuple:
        if part == "odd_num":
            expression_parts.append(str(odd_num_val))
        elif isinstance(part, str) and part in ai_core_parameters:
            expression_parts.append(str(ai_core_parameters[part]))
        elif isinstance(part, tuple) and len(part) == 3 and part[1] in ['+', '-', '*', '/', '**', '%', '//']:
            allowed_operators_map = {
                '+': operator.add, '-': operator.sub, '*': operator.mul, '/': operator.truediv,
                '**': operator.pow, '%': operator.mod, '//': operator.floordiv
            }
            if part[1] not in allowed_operators_map:
                logger_x6_plus_2.error("Attempted to use disallowed operator '%s' in nested formula.", part[1])
                shared_data_register["x6_plus_2_recent_terms_history"].append(float('inf'))
                return float('inf')

            val1 = ai_core_parameters.get(part[0], part[0])
            op_func = allowed_operators_map[part[1]]
            val2 = ai_core_parameters.get(part[2], part[2])

            try:
                if op_func in [operator.pow, operator.mod, operator.floordiv]:
                    nested_result = op_func(int(round(val1)), int(round(val2)))
                else:
                    nested_result = op_func(val1, val2)

                expression_parts.append(f"({nested_result})")
            except ZeroDivisionError:
                logger_x6_plus_2.error("Error: Division by zero in nested symbolic formula: %s. Formula: %s, Part: %s", ZeroDivisionError, current_formula_tuple, part)
                shared_data_register["x6_plus_2_recent_terms_history"].append(float('inf'))
                return float('inf')
            except Exception as e:
                logger_x6_plus_2.error("Error evaluating nested symbolic formula: %s. Formula: %s, Expression: %s", e, current_formula_tuple, expression_str)
                shared_data_register["x6_plus_2_recent_terms_history"].append(-1000000000)
                return -1000000000
        else:
            expression_parts.append(str(part))

    expression_str = " ".join(expression_parts)

    try:
        term = eval(expression_str, {'__builtins__': None, 'odd_num': odd_num_val,
                                     'BASE_FACTOR_A': ai_core_parameters['BASE_FACTOR_A'],
                                     'BASE_FACTOR_B': ai_core_parameters['BASE_FACTOR_B'],
                                     'BASE_FACTOR_C': ai_core_parameters['BASE_FACTOR_C'],
                                     'BASE_FACTOR_D': ai_core_parameters['BASE_FACTOR_D'],
                                    })
    except ZeroDivisionError:
        logger_x6_plus_2.error("Error: Division by zero in x6+2 symbolic formula: %s. Formula: %s, Expression: %s", ZeroDivisionError, current_formula_tuple, expression_str)
        shared_data_register["x6_plus_2_recent_terms_history"].append(float('inf'))
        return float('inf')
    except Exception as e:
        logger_x6_plus_2.error("Error evaluating x6+2 symbolic formula: %s. Formula: %s, Expression: %s", e, current_formula_tuple, expression_str)
        shared_data_register["x6_plus_2_recent_terms_history"].append(-1000000000)
        return -1000000000

    if term in [8, 56, 104]:
        shared_data_register["last_shared_output_value"] = term
        shared_data_register["last_shared_output_source"] = "x6_plus_2_sequence"
        logger_x6_plus_2.info("Detected Shared Output: %s", term)

    shared_data_register["current_x6_plus_2_term"] = term

    shared_data_register["x6_plus_2_recent_terms_history"].append(term)
    if len(shared_data_register["x6_plus_2_recent_terms_history"]) > 5:
        shared_data_register["x6_plus_2_recent_terms_history"].pop(0)

    return int(round(term))

def calculate_25k_target(k: int) -> int:
    logger_main.debug("calculate_25k_target(k=%s) called with C=%s, D=%s", k, ai_core_parameters["BASE_FACTOR_C"], ai_core_parameters["BASE_FACTOR_D"])
    if k < 1: raise ValueError("k must be a positive integer starting from 1.")

    factor_C_int = int(round(ai_core_parameters["BASE_FACTOR_C"]))
    factor_D_int = int(round(ai_core_parameters["BASE_FACTOR_D"]))

    result = (factor_C_int * factor_D_int * k) + k
    logger_main.debug("  25k: Intermediate result (C*D*k)+k: %s * %s * %s + %s = %s", factor_C_int, factor_D_int, k, k, result)
    final_result = int(round(float(result)))
    logger_main.debug("  25k: Final result %s", final_result)
    return final_result

def calculate_decimal_series_term(k: int) -> Fraction:
    logger_main.debug("calculate_decimal_series_term(k=%s) called with C=%s, D=%s", k, ai_core_parameters["BASE_FACTOR_C"], ai_core_parameters["BASE_FACTOR_D"])
    if k < 1: raise ValueError("k must be a positive integer starting from 1.")

    factor_C_int = int(round(ai_core_parameters["BASE_FACTOR_C"]))
    factor_D_int = int(round(ai_core_parameters["BASE_FACTOR_D"]))

    if factor_D_int * k == 0:
        logger_main.debug("  Decimal Series: Division by zero avoided for k=%s. Returning Fraction(0).", k)
        return Fraction(0)

    term_3k_denominator = Fraction(factor_D_int * k)
    result = Fraction(factor_C_int) / term_3k_denominator - Fraction(k)
    logger_main.debug("  Decimal Series: Result C/(D*k) - k: %s / (%s * %s) - %s = %s", factor_C_int, factor_D_int, k, k, result)
    return result

def validate_internal_state(context: str = "general"):
    logger_validator.info("Performing validation check (%s)...", context)
    local_validation_ok = True
    local_error_messages = []
    k_to_validate = shared_data_register["validation_context_k_index"]

    try:
        expected_25k_value = k_to_validate * 25
        calculated_25k = calculate_25k_target(k_to_validate)
        if not math.isclose(float(calculated_25k), float(expected_25k_value), rel_tol=1e-9, abs_tol=0.0):
            local_validation_ok = False
            local_error_messages.append(f"25k Target Mismatch: Expected {expected_25k_value} but calculated {calculated_25k} (k={k_to_validate})")
            logger_validator.error(f"Validation FAILED: 25k Target Mismatch for k=%s. Expected %s, Calculated %s", k_to_validate, expected_25k_value, calculated_25k)
        else:
            logger_validator.info(f"Validation PASSED: 25k Target for k=%s. Calculated %s.", k_to_validate, calculated_25k)
    except ValueError as e:
        local_validation_ok = False
        local_error_messages.append(f"25k Target Calculation Error (k={k_to_validate}): {e}")
        logger_validator.error(f"Validation FAILED: 25k Target Calculation Error for k=%s: %s", k_to_validate, e)
    except ZeroDivisionError as e:
        local_validation_ok = False
        local_error_messages.append(f"25k Target Calculation Error (k={k_to_validate}): Division by zero - {e}")
        logger_validator.error(f"Validation FAILED: Decimal Series Calculation Error for k=%s: Division by zero - %s", k_to_validate, e)


    try:
        calculated_decimal = calculate_decimal_series_term(k_to_validate)
        expected_decimals = {
            1: Fraction(8, 3) - Fraction(1),
            2: Fraction(8, 6) - Fraction(2),
            3: Fraction(8, 9) - Fraction(3),
            4: Fraction(8, 12) - Fraction(4),
            5: Fraction(8, 15) - Fraction(5),
            6: Fraction(8, 18) - Fraction(6)
        }
        if k_to_validate in expected_decimals:
            if not math.isclose(float(calculated_decimal), float(expected_decimals[k_to_validate]), rel_tol=1e-9, abs_tol=0.0):
                 local_validation_ok = False
                 local_error_messages.append(f"Decimal Series Mismatch: k={k_to_validate}, Calculated={calculated_decimal}, Expected={expected_decimals[k_to_validate]}")
                 logger_validator.warning(f"Validation FAILED: Decimal Series Mismatch for k=%s. Expected %s, Calculated %s", k_to_validate, expected_decimals[k_to_validate], calculated_decimal)
            else:
                logger_validator.info(f"Validation PASSED: Decimal Series for k=%s. Calculated %s.", k_to_validate, calculated_decimal)
        else:
            logger_validator.info("Decimal Series k=%s is beyond hardcoded expected values. Calculated: %s", k_to_validate, calculated_decimal)
    except ValueError as e:
        local_validation_ok = False
        local_error_messages.append(f"Decimal Series Calculation Error (k={k_to_validate}): {e}")
        logger_validator.error(f"Validation FAILED: Decimal Series Calculation Error for k=%s: %s", k_to_validate, e)
    except ZeroDivisionError as e:
        local_validation_ok = False
        local_error_messages.append(f"Decimal Series Calculation Error (k={k_to_validate}): Division by zero - {e}")
        logger_validator.error(f"Validation FAILED: Decimal Series Calculation Error for k=%s: Division by zero - %s", k_to_validate, e)


    if not local_validation_ok:
        shared_data_register["validation_flag"] = False
        for err in local_error_messages:
            if err not in shared_data_register["errors_detected"]:
                shared_data_register["errors_detected"].append(err)
        logger_validator.error("Validation FAILED! Errors: %s", local_error_messages)
    else:
        if shared_data_register["validation_flag"] and not local_error_messages:
            shared_data_register["validation_flag"] = True
            shared_data_register["errors_detected"].clear()
        logger_validator.info("Validation PASSED for current checks.")

    if context == "iteration_end":
        shared_data_register["validation_context_k_index"] += 1
        if shared_data_register["validation_context_k_index"] > 6:
            shared_data_register["validation_context_k_index"] = 1

def check_derived_relationships():
    logger_derived_rels.info("Checking core derivations...")
    local_derived_ok = True
    local_error_messages = []

    current_factor_A = ai_core_parameters["BASE_FACTOR_A"]
    current_factor_B = ai_core_parameters["BASE_FACTOR_B"]
    current_factor_C = ai_core_parameters["BASE_FACTOR_C"]

    expected_common_diff = 12.0
    calculated_common_diff = current_factor_A * current_factor_B
    if not math.isclose(calculated_common_diff, expected_common_diff, rel_tol=1e-9, abs_tol=0.0):
        local_derived_ok = False
        local_error_messages.append(f"Derived Rel. Error: {current_factor_A} * {current_factor_B} = {calculated_common_diff}, expected {expected_common_diff}")
        logger_derived_rels.error("Derived Rel. Error: %s * %s = %s, expected %s", current_factor_A, current_factor_B, calculated_common_diff, expected_common_diff)
    else:
        logger_derived_rels.info("%s + %s = 8 (Alternative Form of 8) check PASSED.", current_factor_A, current_factor_B)

    expected_alternative_8 = 8.0
    calculated_alternative_8 = current_factor_A + current_factor_B
    if not math.isclose(calculated_alternative_8, expected_alternative_8, rel_tol=1e-9, abs_tol=0.0):
        local_derived_ok = False
        local_error_messages.append(f"Derived Rel. Error: {current_factor_A} + {current_factor_B} = {calculated_alternative_8}, expected {expected_alternative_8}")
        logger_derived_rels.error("Derived Rel. Error: %s + %s = %s, expected %s", current_factor_A, current_factor_B, calculated_alternative_8, expected_alternative_8)
    else:
        logger_derived_rels.info("%s + %s = 8 (Alternative Form of 8) check PASSED.", current_factor_A, current_factor_B)

    expected_104_derived = 104.0
    calculated_104_derived = 56.0 * current_factor_A - current_factor_C
    if not math.isclose(calculated_104_derived, expected_104_derived, rel_tol=1e-9, abs_tol=0.0):
        local_derived_ok = False
        local_error_messages.append(f"Derived Rel. Error: 56 * {current_factor_A} - {current_factor_C} = {calculated_104_derived}, expected {expected_104_derived}")
        logger_derived_rels.error(f"Derived Rel. Error: 56 * %s - %s = %s, expected %s", current_factor_A, current_factor_C, calculated_104_derived, expected_104_derived)
    else:
        logger_derived_rels.info("56 * %s - %s = 104 (Shared Output Interconnection) check PASSED.", current_factor_A, current_factor_C)

    if not local_derived_ok:
        shared_data_register["validation_flag"] = False
        for err in local_error_messages:
            if err not in shared_data_register["errors_detected"]:
                shared_data_register["errors_detected"].append(err)
        logger_derived_rels.error("Derived Relationships Validation FAILED! Overall system validation_flag set to False.")
    else:
        if shared_data_register["validation_flag"] and not local_error_messages:
            shared_data_register["validation_flag"] = True
            shared_data_register["errors_detected"].clear()
        logger_derived_rels.info("All Derived Relationships checks PASSED.")

def coordinate_on_shared_output():
    last_val = shared_data_register["last_shared_output_value"]
    last_src = shared_data_register["last_shared_output_source"]

    if last_val is not None:
        logger_coordinator.info("Detected Global Shared Output: %s from %s", last_val, last_src)

        if last_val == 8:
            logger_coordinator.info("Action: Initializing core systems (8 detected).")
            validate_internal_state(context="initial_8_detection")
        elif last_val == 56:
            logger_coordinator.info("Action: Verifying module alignment (56 detected).")
            validate_internal_state(context="56_alignment")
        elif last_val == 104:
            logger_coordinator.info("Action: Advancing to next operational phase (104 detected).")
            validate_internal_state(context="phase_advance_104")
            check_derived_relationships()

        shared_data_register["last_shared_output_value"] = None
        shared_data_register["last_shared_output_source"] = None
    else:
        logger_coordinator.info("No new shared output detected in this cycle.")

def calculate_overall_system_loss() -> float:
    overall_loss_score = 0.0

    # --- Part 1: Core Derivations Disharmony ---
    current_factor_A = ai_core_parameters["BASE_FACTOR_A"]
    current_factor_B = ai_core_parameters["BASE_FACTOR_B"]
    current_factor_C = ai_core_parameters["BASE_FACTOR_C"]

    derived_rel_errors_sum_sq = (float(current_factor_A * current_factor_B) - 12.0) ** 2 + \
                                (float(current_factor_A + current_factor_B) - 8.0) ** 2 + \
                                (float(56.0 * current_factor_A - current_factor_C) - 104.0) ** 2
    overall_loss_score += derived_rel_errors_sum_sq * ai_value_hierarchy["CORE_DERIVATIONS_DISHARMONY_WEIGHT"]

    # --- Part 2: Decimal Series Disharmony ---
    expected_decimals = {
        1: Fraction(8, 3) - Fraction(1),
        2: Fraction(8, 6) - Fraction(2),
        3: Fraction(8, 9) - Fraction(3),
        4: Fraction(8, 12) - Fraction(4),
            5: Fraction(8, 15) - Fraction(5),
        6: Fraction(8, 18) - Fraction(6)
    }

    decimal_series_sum_sq_errors = 0.0
    for k_val in range(1, 7):
        try:
            calculated_decimal = calculate_decimal_series_term(k_val)
            expected_decimal = expected_decimals[k_val]

            error = float(calculated_decimal - expected_decimal)
            decimal_series_sum_sq_errors += error ** 2
        except Exception as e:
            overall_loss_score += 1000.0
            logger_main.error("Harmony Score: Error calculating Decimal Series for k=%s: %s", k_val, e)
    overall_loss_score += decimal_series_sum_sq_errors * ai_value_hierarchy["DECIMAL_SERIES_DISHARMONY_WEIGHT"]

    # --- Part 3: x6+2 First Term Goal Disharmony ---
    original_x6_plus_2_formula_for_loss = veritas_core_formulas["x6+2_sequence"]
    current_first_x6_plus_2_term = generate_x6_plus_2_sequence_term(1)
    # Restore the formula after calculating the term for loss calculation
    veritas_core_formulas["x6+2_sequence"] = original_x6_plus_2_formula_for_loss


    target_first_x6_plus_2_term = shared_data_register["goal_for_x6_plus_2_first_term"]

    first_term_error = (float(current_first_x6_plus_2_term) - float(target_first_x6_plus_2_term)) ** 2
    overall_loss_score += first_term_error * ai_value_hierarchy["X6_PLUS_2_FIRST_TERM_DISHARMONY_WEIGHT"]

    # --- Part 4: Generic 25k Disharmony ---
    first_25k_term = calculate_25k_target(1)
    expected_25k_at_k1 = 25
    if not math.isclose(float(first_25k_term), float(expected_25k_at_k1), rel_tol=1e-9, abs_tol=0.0):
        overall_loss_score += ai_value_hierarchy["GENERIC_25K_DISHARMONY_WEIGHT"] * 1000.0
        logger_main.warning("Harmony Score: 25k Goal Mismatch! Calculated: %s, Expected: %s. Applying high penalty.", first_25k_term, expected_25k_at_k1)
    elif first_25k_term % 25 != 0:
        overall_loss_score += ai_value_hierarchy["GENERIC_25K_DISHARMONY_WEIGHT"] * 1000.0
        logger_main.warning("Harmony Score: 25k Goal Mismatch! Calculated: %s (not multiple of 25)", first_25k_term)

    return overall_loss_score


def attempt_self_correction(max_attempts: int = 100, learning_rate: float = 1.0) -> bool:
    logger_main.info("\n[Self-Correction Module] Initiating sophisticated multi-parameter self-correction process...")

    current_attempt = 0
    harmony_score_tolerance = 1e-6

    while current_attempt < max_attempts:
        current_attempt += 1

        original_params_at_attempt_start = ai_core_parameters.copy()

        current_overall_loss = calculate_overall_system_loss()
        logger_main.info("  [Self-Correction] Attempt %s: A=%s, B=%s, C=%s, D=%s | Overall Loss: %s",
                         current_attempt, ai_core_parameters["BASE_FACTOR_A"],
                         ai_core_parameters["BASE_FACTOR_B"], ai_core_parameters["BASE_FACTOR_C"],
                         ai_core_parameters["BASE_FACTOR_D"], current_overall_loss)


        if current_overall_loss < harmony_score_tolerance:
            logger_main.info("[Self-Correction Module] Self-correction successful! Overall Loss is %s (near 0).", current_overall_loss)
            shared_data_register["validation_flag"] = True
            shared_data_register["errors_detected"].clear()
            return True

        best_nudge_candidate = None
        min_loss_found_in_nudge = current_overall_loss

        adjustable_params = ["BASE_FACTOR_A", "BASE_FACTOR_B", "BASE_FACTOR_C", "BASE_FACTOR_D"]

        scores_for_logging = {}

        for param_name in adjustable_params:
            original_param_value = original_params_at_attempt_start[param_name]

            # Nudge UP
            ai_core_parameters[param_name] = original_param_value + learning_rate
            score_up = calculate_overall_system_loss()
            scores_for_logging[f"{param_name}+"] = score_up
            if score_up < min_loss_found_in_nudge:
                min_loss_found_in_nudge = score_up
                best_nudge_candidate = (param_name, learning_rate)

            # Nudge DOWN
            ai_core_parameters[param_name] = original_param_value - learning_rate
            score_down = calculate_overall_system_loss()
            scores_for_logging[f"{param_name}-"] = score_down
            if score_down < min_loss_found_in_nudge:
                min_loss_found_in_nudge = score_down
                best_nudge_candidate = (param_name, -learning_rate)

            # Restore parameter to original value for next test
            ai_core_parameters[param_name] = original_param_value

        logger_main.info("    Tested Nudges: %s", ", ".join([f"{p}: {s:.3f}" for p,s in scores_for_logging.items()]))


        if best_nudge_candidate is None:
            logger_main.warning("[Self-Correction Module] Stuck! No immediate improvement found with current learning_rate. Overall Loss: %s", current_overall_loss)
            return False

        param_to_adjust, adjustment_value = best_nudge_candidate
        ai_core_parameters[param_to_adjust] += adjustment_value
        logger_main.info("    Adjusting %s: %s to %s", param_to_adjust, adjustment_value, ai_core_parameters[param_to_adjust])

    logger_main.error("[Self-Correction Module] Self-correction FAILED after %s attempts. Could not restore consistency.", max_attempts)
    return False

# Placeholder for the meta-rule experimentation function
def experiment_with_meta_rule(rule_name: str, meta_rule_function, reset_params_to_default: bool) -> bool:
    """
    Function to simulate experimenting with a meta-rule.
    It applies the meta_rule_function to the specified rule,
    tests the system's harmony/loss with the new rule,
    and decides whether to keep it based on loss reduction.
    If reset_params_to_default is True and the rule is adopted,
    it attempts self-correction with the new rule and reset parameters.
    """
    logger_meta_rule.info(f"\n[MetaRule Experimentation] Attempting to experiment with meta-rule for '{rule_name}'...")
    logger_meta_rule.info(f"  Meta-rule function: {meta_rule_function.__name__}")
    logger_meta_rule.info(f"  Reset params to default: {reset_params_to_default}")

    original_formula = veritas_core_formulas.get(rule_name)
    if original_formula is None:
        logger_meta_rule.warning(f"  Rule '{rule_name}' not found in veritas_core_formulas.")
        return False

    logger_meta_rule.info(f"  Original formula for '{rule_name}': {original_formula}")
    original_params_state = ai_core_parameters.copy()
    original_overall_loss = calculate_overall_system_loss()
    logger_meta_rule.info(f"  Original Overall Loss: {original_overall_loss}")

    try:
        # --- Apply the meta-rule function hypothetically ---
        hypothetical_new_formula = meta_rule_function(original_formula)
        logger_meta_rule.info(f"  Hypothetical new formula: {hypothetical_new_formula}")

        # Avoid testing if the meta-rule didn't actually change the formula
        if hypothetical_new_formula == original_formula:
            logger_meta_rule.info("  Meta-rule did not change the formula. No experiment needed.")
            return False

        # --- Temporarily test the new formula ---
        # Store current formula and parameters to restore later
        current_formula_state = veritas_core_formulas[rule_name]
        current_params_state = ai_core_parameters.copy()

        veritas_core_formulas[rule_name] = hypothetical_new_formula
        if reset_params_to_default:
             # Reset parameters to initial ideal values for a fair test with a new rule structure
             ai_core_parameters.update({
                "BASE_FACTOR_A": 2.0, "BASE_FACTOR_B": 6.0,
                "BASE_FACTOR_C": 8.0, "BASE_FACTOR_D": 3.0
             })
             logger_meta_rule.info("  Resetting parameters to ideal defaults for testing new rule.")


        # Calculate loss with the hypothetical new formula and potentially reset parameters
        loss_with_new_formula = calculate_overall_system_loss()
        logger_meta_rule.info(f"  Overall Loss with hypothetical new formula: {loss_with_new_formula}")


        # --- Decide whether to adopt the new rule ---
        # Adoption criteria: Is the loss significantly reduced compared to the original loss?
        adoption_threshold = 0.1 * original_overall_loss # Example threshold: at least 10% reduction
        # Ensure we don't adopt if the new loss is actually higher or not significantly lower
        if loss_with_new_formula < original_overall_loss - adoption_threshold:
             logger_meta_rule.info("  Hypothetical new formula shows significant improvement! Adopting.")
             # Adopt the new formula permanently for the rest of this run
             # veritas_core_formulas[rule_name] = hypothetical_new_formula # Already temporarily set above, now it's permanent as we won't restore the old one below
             logger_meta_rule.info(f"  New formula for '{rule_name}' adopted: {veritas_core_formulas[rule_name]}")

             # If reset_params_to_default was requested, attempt self-correction with the new rule
             if reset_params_to_default:
                 logger_meta_rule.info("  Attempting self-correction with new rule and reset parameters...")
                 # Note: The success of this self-correction isn't the only factor for meta-rule adoption
                 # but it's a way to see if the new rule allows finding a better minimum.
                 attempt_self_correction(max_attempts=50, learning_rate=0.5) # Attempt to fine-tune

             return True # Indicate successful adoption
        else:
            logger_meta_rule.info("  Hypothetical new formula did not show significant improvement or increased loss.")
            # --- Restore original formula and parameters if the new rule is not adopted ---
            veritas_core_formulas[rule_name] = current_formula_state
            ai_core_parameters.update(current_params_state)
            logger_meta_rule.info("  Restored original formula and parameters.")


    except Exception as e:
        logger_meta_rule.error(f"  Error during meta-rule experimentation: {e}")
        # Ensure state is restored even if an error occurs
        veritas_core_formulas[rule_name] = original_formula
        ai_core_parameters.update(original_params_state)
        logger_meta_rule.warning("  State restored after error.")
        return False

    logger_meta_rule.info("[MetaRule Experimentation] Experiment completed. Rule not adopted in this instance.")
    return False # Indicate that no rule was permanently adopted in this placeholder version


def apply_operator_swap_meta_rule(formula_tuple: tuple) -> tuple:
    """
    Swaps the positions of the first multiplication and first addition operator.
    Example: ('odd_num', '*', 'BASE_FACTOR_B', '+', 'BASE_FACTOR_A') -> ('odd_num', '+', 'BASE_FACTOR_B', '*', 'BASE_FACTOR_A')
    """
    if not (len(formula_tuple) == 5 and
            isinstance(formula_tuple[1], str) and isinstance(formula_tuple[3], str) and
            all(op in ['*', '+'] for op in [formula_tuple[1], formula_tuple[3]])):
        logger_meta_rule.error("Invalid formula tuple format for operator swap: %s", formula_tuple)
        return formula_tuple

    op1, operator1, op2, operator2, op3 = formula_tuple

    # Check if the operators are indeed multiplication and addition in the expected order
    if operator1 == '*' and operator2 == '+':
        new_formula = (op1, operator2, op2, operator1, op3)
        logger_meta_rule.debug("  Applied operator swap: %s -> %s", formula_tuple, new_formula)
        return new_formula
    # Optional: also handle the reverse case if needed, but the current structure implies a specific form
    # elif operator1 == '+' and operator2 == '*':
    #     new_formula = (op1, operator2, op2, operator1, op3)
    #     logger_meta_rule.debug("  Applied operator swap (reverse): %s -> %s", formula_tuple, new_formula)
    #     return new_formula
    else:
        logger_meta_rule.debug("No direct '*' followed by '+' structure found for operator swap in %s", formula_tuple)
        # If the structure doesn't match, return the original formula to indicate no change
        return formula_tuple


def generate_and_test_factor_combination_rule() -> bool:
    logger_inventor.info("\n[Inventor Module] Generating and testing factor combination rules...")

    original_formula = veritas_core_formulas["x6+2_sequence"]
    original_params_state = ai_core_parameters.copy()
    original_overall_loss = calculate_overall_system_loss()
    logger_inventor.info(f"  Original Overall Loss: {original_overall_loss}")

    adjustable_params = list(ai_core_parameters.keys())
    operators = ['+', '-', '*', '/'] # Consider basic operators for combination

    best_formula_found = None
    min_loss_found = original_overall_loss
    best_params_with_new_formula = None


    # Simple approach: Try replacing parts of the existing formula
    # This is a basic example; a real system would explore more complex structures
    current_formula_list = list(veritas_core_formulas["x6+2_sequence"])

    for i in range(len(current_formula_list)):
        original_part = current_formula_list[i]

        # Try replacing with parameters
        for param_name in adjustable_params:
             current_formula_list[i] = param_name
             hypothetical_formula = tuple(current_formula_list)
             logger_inventor.debug(f"  Testing replacement with parameter: {hypothetical_formula}")

             # Temporarily set the hypothetical formula and reset params
             temp_formula_state = veritas_core_formulas["x6+2_sequence"]
             temp_params_state = ai_core_parameters.copy()
             veritas_core_formulas["x6+2_sequence"] = hypothetical_formula
             ai_core_parameters.update({
                "BASE_FACTOR_A": 2.0, "BASE_FACTOR_B": 6.0,
                "BASE_FACTOR_C": 8.0, "BASE_FACTOR_D": 3.0
             })

             try:
                 loss_with_new_formula = calculate_overall_system_loss()
                 logger_inventor.debug(f"    Loss: {loss_with_new_formula}")

                 if loss_with_new_formula < min_loss_found:
                     min_loss_found = loss_with_new_formula
                     best_formula_found = hypothetical_formula
                     # Attempt self-correction to find the best parameters for this new formula
                     # Note: Running self-correction inside the loop can be computationally expensive
                     # For a basic example, this is acceptable, but for larger searches,
                     # you might defer self-correction until after finding the best candidate formula
                     attempt_self_correction(max_attempts=50, learning_rate=0.5)
                     best_params_with_new_formula = ai_core_parameters.copy()
                     logger_inventor.info(f"  Found potential better formula: {best_formula_found} with loss {min_loss_found} and params {best_params_with_new_formula}")

             except Exception as e:
                 logger_inventor.debug(f"    Error testing formula {hypothetical_formula}: {e}")

             # Restore state
             veritas_core_formulas["x6+2_sequence"] = temp_formula_state
             ai_core_parameters.update(temp_params_state)

        # Try replacing with operators (only if the original part was an operator)
        if isinstance(original_part, str) and original_part in operators:
             for op in operators:
                 current_formula_list[i] = op
                 hypothetical_formula = tuple(current_formula_list)
                 logger_inventor.debug(f"  Testing replacement with operator: {hypothetical_formula}")

                 # Temporarily set the hypothetical formula and reset params
                 temp_formula_state = veritas_core_formulas["x6+2_sequence"]
                 temp_params_state = ai_core_parameters.copy()
                 veritas_core_formulas["x6+2_sequence"] = hypothetical_formula
                 ai_core_parameters.update({
                    "BASE_FACTOR_A": 2.0, "BASE_FACTOR_B": 6.0,
                    "BASE_FACTOR_C": 8.0, "BASE_FACTOR_D": 3.0
                 })

                 try:
                     loss_with_new_formula = calculate_overall_system_loss()
                     logger_inventor.debug(f"    Loss: {loss_with_new_formula}")

                     if loss_with_new_formula < min_loss_found:
                         min_loss_found = loss_with_new_formula
                         best_formula_found = hypothetical_formula
                         # Attempt self-correction to find the best parameters for this new formula
                         # Note: Running self-correction inside the loop can be computationally expensive
                         # For a basic example, this is acceptable, but for larger searches,
                         # you might defer self-correction until after finding the best candidate formula
                         attempt_self_correction(max_attempts=50, learning_rate=0.5)
                         best_params_with_new_formula = ai_core_parameters.copy()
                         logger_inventor.info(f"  Found potential better formula: {best_formula_found} with loss {min_loss_found} and params {best_params_with_new_formula}")

                 except Exception as e:
                     logger_inventor.debug(f"    Error testing formula {hypothetical_formula}: {e}")

                 # Restore state
                 veritas_core_formulas["x6+2_sequence"] = temp_formula_state
                 ai_core_parameters.update(temp_params_state)

        # Restore the original part before moving to the next index
        current_formula_list[i] = original_part


    # Decide whether to adopt the best formula found
    # Increased adoption threshold to make it harder to adopt and ensure failure for the demo
    adoption_threshold = 0.5 * original_overall_loss # Increased threshold


    if best_formula_found and min_loss_found < original_overall_loss - adoption_threshold:
        logger_inventor.info(f"[Inventor Module] Invention SUCCESS! Adopting new formula: {best_formula_found} with loss {min_loss_found}")
        veritas_core_formulas["x6+2_sequence"] = best_formula_found
        if best_params_with_new_formula:
             ai_core_parameters.update(best_params_with_new_formula)
             logger_inventor.info("  Adopted parameters found during self-correction for the new formula.")
        else:
             # If self-correction failed for the best formula, maybe just reset params to default?
             ai_core_parameters.update({
                "BASE_FACTOR_A": 2.0, "BASE_FACTOR_B": 6.0,
                "BASE_FACTOR_C": 8.0, "BASE_FACTOR_D": 3.0
             })
             logger_inventor.warning("  Self-correction failed for the adopted formula. Resetting parameters to defaults.")

        return True
    else:
        logger_inventor.warning("[Inventor Module] Invention FAILED: No significantly better formula found based on threshold.")
        # Restore original state if no better formula was adopted
        veritas_core_formulas["x6+2_sequence"] = original_formula
        ai_core_parameters.update(original_params_state)
        logger_inventor.info("  Restored original formula and parameters.")
        return False


def detect_x6_plus_2_trend() -> str:
    history = shared_data_register["x6_plus_2_recent_terms_history"]

    if len(history) < 3:
        return "Insufficient Data"

    if all(math.isclose(term, history[0], rel_tol=1e-9) for term in history):
        return "Stuck/Constant"

    diffs = [history[i+1] - history[i] for i in range(len(history) - 1)]

    if all(d > 0 for d in diffs):
        return "Growing"
    if all(d < 0 for d in diffs):
        return "Decaying"

    if len(diffs) >= 2:
        oscillating = True
        for i in range(len(diffs) - 1):
            if (diffs[i] > 0 and diffs[i+1] > 0) or \
               (diffs[i] < 0 and diffs[i+1] < 0) or \
               math.isclose(diffs[i], 0, abs_tol=1e-9):
                oscillating = False
                break
        if oscillating:
            return "Oscillating"

    return "No Clear Trend"


def apply_conceptual_blend(current_loss: float) -> bool:
    logger_main.info("\n[Conceptual Blending Module] Analyzing system state for conceptual leaps...")

    if current_loss > 1e-6:
        trend = detect_x6_plus_2_trend()
        logger_main.info("  Detected trend in x6+2 sequence: %s", trend)

        original_formula = veritas_core_formulas["x6+2_sequence"]
        original_params_state = ai_core_parameters.copy()

        candidate_formulas = []

        if trend in ["Growing", "Decaying"]:
            logger_main.info("  Mapping concept '%s' to mathematical operation 'Exponentiation'.", trend)

            # Hypothesis 1: Replace BASE_FACTOR_B with (BASE_FACTOR_A ** BASE_FACTOR_D)
            hypo_formula_1_list = list(original_formula)
            # Ensure the list is long enough and the indices are valid before modification
            if len(hypo_formula_1_list) > 2:
                hypo_formula_1_list[2] = ("BASE_FACTOR_A", "**", "BASE_FACTOR_D")
                candidate_formulas.append(tuple(hypo_formula_1_list))
            else:
                 logger_main.warning("  Conceptual Blend Hypothesis 1 skipped: Formula list too short for index 2.")


            # Hypothesis 2: Replace BASE_FACTOR_A with (BASE_FACTOR_B ** BASE_FACTOR_D)
            hypo_formula_2_list = list(original_formula)
            # Ensure the list is long enough and the indices are valid before modification
            if len(hypo_formula_2_list) > 4:
                hypo_formula_2_list[4] = ("BASE_FACTOR_B", "**", "BASE_FACTOR_D")
                candidate_formulas.append(tuple(hypo_formula_2_list))
            else:
                 logger_main.warning("  Conceptual Blend Hypothesis 2 skipped: Formula list too short for index 4.")


            # Hypothesis 3: Replace BASE_FACTOR_B with (odd_num ** BASE_FACTOR_D)
            hypo_formula_3_list = list(original_formula)
            # Ensure the list is long enough and the indices are valid before modification
            if len(hypo_formula_3_list) > 2:
                hypo_formula_3_list[2] = ("odd_num", "**", "BASE_FACTOR_D")
                candidate_formulas.append(tuple(hypo_formula_3_list))
            else:
                 logger_main.warning("  Conceptual Blend Hypothesis 3 skipped: Formula list too short for index 2.")

            # --- New Division-based Hypotheses for Growing/Decaying ---
            logger_main.info("  Adding Division-based hypotheses for '%s' trend.", trend)
            # Hypothesis 4: Replace multiplication with division
            hypo_formula_4_list = list(original_formula)
            if len(hypo_formula_4_list) > 1 and hypo_formula_4_list[1] == '*':
                hypo_formula_4_list[1] = '/'
                candidate_formulas.append(tuple(hypo_formula_4_list))
                logger_main.info("  Hypothesis 4 (Multiply -> Divide): %s", tuple(hypo_formula_4_list))
            else:
                logger_main.warning("  Conceptual Blend Hypothesis 4 skipped: Formula does not have '*' at index 1.")

            # Hypothesis 5: Replace addition with division
            hypo_formula_5_list = list(original_formula)
            if len(hypo_formula_5_list) > 3 and hypo_formula_5_list[3] == '+':
                hypo_formula_5_list[3] = '/'
                candidate_formulas.append(tuple(hypo_formula_5_list))
                logger_main.info("  Hypothesis 5 (Add -> Divide): %s", tuple(hypo_formula_5_list))
            else:
                 logger_main.warning("  Conceptual Blend Hypothesis 5 skipped: Formula does not have '+' at index 3.")


        # --- Anti-Stuckness Rule ---
        if trend == "Stuck/Constant":
            logger_main.info("  Detected 'Stuck/Constant' trend. Applying 'Anti-Stuckness' blend.")
            # Example Anti-Stuckness Blend: Add a small constant or multiply by a factor
            anti_stuck_formula_list = list(original_formula)
            # Example: Add BASE_FACTOR_D to the end
            anti_stuck_formula_list.extend(['+', 'BASE_FACTOR_D'])
            candidate_formulas.append(tuple(anti_stuck_formula_list))
            logger_main.info("  Hypothesized 'Anti-Stuckness' formula: %s", tuple(anti_stuck_formula_list))


        for h_formula in candidate_formulas:
            logger_meta_rule.info("  Testing candidate formula: %s", h_formula)
            # Store current state before temporary modification
            current_formula_state = veritas_core_formulas["x6+2_sequence"]
            current_params_state = ai_core_parameters.copy()


            veritas_core_formulas["x6+2_sequence"] = h_formula
            # Reset parameters to initial ideal values for a fair test with a new rule structure
            ai_core_parameters.update({
                "BASE_FACTOR_A": 2.0, "BASE_FACTOR_B": 6.0,
                "BASE_FACTOR_C": 8.0, "BASE_FACTOR_D": 3.0
             })
            logger_meta_rule.info("  Resetting parameters to ideal defaults for testing new blend formula.")


            correction_successful = attempt_self_correction(max_attempts=50, learning_rate=1.0)

            if correction_successful:
                logger_main.info("[Conceptual Blending Module] Conceptual Blend SUCCESS! New rule adopted: %s", h_formula)
                # Keep the new formula and the parameters found by self-correction
                return True
            else:
                logger_main.warning("[Conceptual Blending Module] Conceptual Blend FAILED to achieve harmony for %s. Reverting.", h_formula)
                # Restore original formula and parameters if the blend didn't lead to harmony
                veritas_core_formulas["x6+2_sequence"] = original_formula
                ai_core_parameters.update(original_params_state)

        logger_main.warning("[Conceptual Blending Module] No beneficial conceptual blend found or applied.")

    return False


# --- Main Simulation Loop ---
logger_main.info("\n--- Simulating Parameter Adjustment for Learning ---")

target_first_x6_plus_2_term = shared_data_register["goal_for_x6_plus_2_first_term"]
logger_main.info("Learning Goal: Make the first term of x6+2 sequence = %s", target_first_x6_plus_2_term)

# --- Set up initial conflict for this demo: HIGH PRIORITY CONFLICTS ---
ai_core_parameters["BASE_FACTOR_A"] = 4.0
ai_core_parameters["BASE_FACTOR_B"] = 6.0
ai_core_parameters["BASE_FACTOR_C"] = 8.0
ai_core_parameters["BASE_FACTOR_D"] = 3.0


logger_main.info("Initial AI Core Parameters (PERTURBED for Value Conflict Demo): %s", ai_core_parameters)
logger_main.info("Initial Overall Loss (PERTURBED State): %s", calculate_overall_system_loss())

logger_main.info("\n--- Running System with ADJUSTED (Perturbed) Parameters ---")

shared_data_register["errors_detected"].clear()
shared_data_register["validation_flag"] = True
shared_data_register["validation_context_k_index"] = 1
shared_data_register["x6_plus_2_recent_terms_history"].clear()

for i in range(1, 10):
    logger_main.info("\n--- Iteration %s ---", i)

    shared_data_register["errors_detected"].clear()
    shared_data_register["validation_flag"] = True

    _ = generate_x8_progression_term(i)
    coordinate_on_shared_output()
    _ = generate_x6_plus_2_sequence_term(i)
    coordinate_on_shared_output()

    validate_internal_state(context="iteration_end")

# --- Post-Main Loop Self-Correction & Invention Logic ---
logger_main.info("\n--- Post-Main Loop Self-Correction & Invention Attempt ---")

shared_data_register["errors_detected"].clear()
shared_data_register["validation_flag"] = True
shared_data_register["validation_context_k_index"] = 1

validate_internal_state(context="final_aggregate_check")
check_derived_relationships()


if not shared_data_register["validation_flag"] and len(shared_data_register["errors_detected"]) > 0:
    logger_main.error("[AI_Core] Critical validation failure(s) detected in main run. Initiating self-correction!")

    correction_successful = attempt_self_correction(max_attempts=10, learning_rate=1.0)

    if correction_successful:
        logger_main.info("[AI_Core] Self-correction successful! System should now be consistent.")

        shared_data_register["errors_detected"].clear()
        shared_data_register["validation_flag"] = True
        shared_data_register["validation_context_k_index"] = 1

        validate_internal_state(context="post_correction_final_confirmation")
        check_derived_relationships()

        logger_meta_rule.info("\n[AI_Core] Attempting Meta-Rule Experimentation (Operator Swap) after successful self-correction...")
        meta_rule_success = experiment_with_meta_rule(
            rule_name="x6+2_sequence",
            meta_rule_function=apply_operator_swap_meta_rule,
            reset_params_to_default=True
        )

        if meta_rule_success:
            logger_main.info("[AI_Core] Meta-Rule Experimentation SUCCESS: New rule adopted! System has evolved structurally.")
        else:
            logger_main.warning("[AI_Core] Meta-Rule Experimentation FAILED: Rule not adopted. System remains with original rule.")

    else:
        logger_main.critical("[AI_Core] Self-correction FAILED! System remains inconsistent.")
        logger_main.info("\n[Inventor Module] Parameter adjustment failed. Initiating broader Rule Invention...")

        logger_meta_rule.info("\n[AI_Core] Attempting Meta-Rule Experimentation (Operator Swap) after failed self-correction...")
        meta_rule_success_ops_swap = experiment_with_meta_rule(
            rule_name="x6+2_sequence",
            meta_rule_function=apply_operator_swap_meta_rule,
            reset_params_to_default=True
        )

        if meta_rule_success_ops_swap:
            logger_main.info("[AI_Core] Meta-Rule Experimentation (Operator Swap) SUCCESS: New rule adopted! System has evolved structurally.")
        else:
            logger_main.warning("[AI_Core] Meta-Rule Experimentation (Operator Swap) FAILED: Rule not adopted. System remains with original rule.")
            logger_main.info("\n[Inventor Module] Meta-Rule Experimentation failed. Attempting Factor-Combination Invention...")

            invention_successful = generate_and_test_factor_combination_rule()

            if invention_successful:
                logger_main.info("[AI_Core] Novel Rule Invention (Factor-Combination) SUCCESS: New rule adopted! System has evolved structurally.")
            else:
                logger_main.critical("[AI_Core] Novel Rule Invention (Factor-Combination) FAILED: Could not find beneficial rule. System remains fundamentally inconsistent.")
                # NEW: Attempt Conceptual Blending if all else fails
                logger_main.info("\n[Conceptual Blending Module] All structured invention failed. Attempting conceptual leap...")
                conceptual_blend_successful = apply_conceptual_blend(calculate_overall_system_loss())

                if conceptual_blend_successful:
                    logger_main.info("[AI_Core] Conceptual Blending SUCCESS! System has evolved conceptually.")
                else:
                    logger_main.critical("[AI_Core] Conceptual Blending FAILED. System remains fundamentally inconsistent.")


else:
    logger_main.info("[AI_Core] No critical validation failure detected in main run. Self-correction not triggered.")

# Move final logging statements before closing the file
logger_main.info(f"Script execution complete. Full log saved to {full_output_path}")
logger_main.info("\nFinal state of shared_data_register after full sequence run with self-correction attempt: %s", shared_data_register)
logger_main.info("Final AI Core Parameters after self-correction attempt: %s", ai_core_parameters)
logger_main.info("Final Veritas Core Formulas (symbolic): %s", veritas_core_formulas)

sys.stdout = original_stdout
log_file.close()